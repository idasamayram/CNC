{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b7c276d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'SlidingWindowTransformer' from 'sklearn.preprocessing' (/home/fe/asadi/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data_loader_utils\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SlidingWindowTransformer\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'SlidingWindowTransformer' from 'sklearn.preprocessing' (/home/fe/asadi/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/__init__.py)"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "from tsfresh import extract_features\n",
    "from tsfresh import select_features\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "import os\n",
    "from utils import data_loader_utils\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import SlidingWindowTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59963409-49bf-4d22-96e0-388e4d29808a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils import data_loader_utils\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tsfresh import extract_features\n",
    "from tsfresh import select_features\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "717e107b",
   "metadata": {},
   "outputs": [],
   "source": [
    "machines = [\"M01\",\"M02\",\"M03\"]\n",
    "process_names = [\"OP00\",\"OP01\",\"OP02\",\"OP03\",\"OP04\",\"OP05\",\"OP06\",\"OP07\",\"OP08\",\"OP09\",\"OP10\",\"OP11\",\"OP12\",\"OP13\",\"OP14\"]\n",
    "labels = [\"good\",\"bad\"]\n",
    "path_to_dataset = \"./data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f4e44cc-4f57-401e-92bd-8b174a51104f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "laoding files from ./data/M01/OP00/good... \n",
      "laoding files from ./data/M01/OP00/bad... \n",
      "laoding files from ./data/M02/OP00/good... \n",
      "laoding files from ./data/M02/OP00/bad... \n",
      "laoding files from ./data/M03/OP00/good... \n",
      "laoding files from ./data/M03/OP00/bad... \n",
      "laoding files from ./data/M01/OP01/good... \n",
      "laoding files from ./data/M01/OP01/bad... \n",
      "laoding files from ./data/M02/OP01/good... \n",
      "laoding files from ./data/M02/OP01/bad... \n",
      "laoding files from ./data/M03/OP01/good... \n",
      "laoding files from ./data/M03/OP01/bad... \n",
      "laoding files from ./data/M01/OP02/good... \n",
      "laoding files from ./data/M01/OP02/bad... \n",
      "laoding files from ./data/M02/OP02/good... \n",
      "laoding files from ./data/M02/OP02/bad... \n",
      "laoding files from ./data/M03/OP02/good... \n",
      "laoding files from ./data/M03/OP02/bad... \n",
      "laoding files from ./data/M01/OP03/good... \n",
      "laoding files from ./data/M01/OP03/bad... \n",
      "laoding files from ./data/M02/OP03/good... \n",
      "laoding files from ./data/M02/OP03/bad... \n",
      "laoding files from ./data/M03/OP03/good... \n",
      "laoding files from ./data/M03/OP03/bad... \n",
      "laoding files from ./data/M01/OP04/good... \n",
      "laoding files from ./data/M01/OP04/bad... \n",
      "laoding files from ./data/M02/OP04/good... \n",
      "laoding files from ./data/M02/OP04/bad... \n",
      "laoding files from ./data/M03/OP04/good... \n",
      "laoding files from ./data/M03/OP04/bad... \n",
      "laoding files from ./data/M01/OP05/good... \n",
      "laoding files from ./data/M01/OP05/bad... \n",
      "laoding files from ./data/M02/OP05/good... \n",
      "laoding files from ./data/M02/OP05/bad... \n",
      "laoding files from ./data/M03/OP05/good... \n",
      "laoding files from ./data/M03/OP05/bad... \n",
      "laoding files from ./data/M01/OP06/good... \n",
      "laoding files from ./data/M01/OP06/bad... \n",
      "laoding files from ./data/M02/OP06/good... \n",
      "laoding files from ./data/M02/OP06/bad... \n",
      "laoding files from ./data/M03/OP06/good... \n",
      "laoding files from ./data/M03/OP06/bad... \n",
      "laoding files from ./data/M01/OP07/good... \n",
      "laoding files from ./data/M01/OP07/bad... \n",
      "laoding files from ./data/M02/OP07/good... \n",
      "laoding files from ./data/M02/OP07/bad... \n",
      "laoding files from ./data/M03/OP07/good... \n",
      "laoding files from ./data/M03/OP07/bad... \n",
      "laoding files from ./data/M01/OP08/good... \n",
      "laoding files from ./data/M01/OP08/bad... \n",
      "laoding files from ./data/M02/OP08/good... \n",
      "laoding files from ./data/M02/OP08/bad... \n",
      "laoding files from ./data/M03/OP08/good... \n",
      "laoding files from ./data/M03/OP08/bad... \n",
      "laoding files from ./data/M01/OP09/good... \n",
      "laoding files from ./data/M01/OP09/bad... \n",
      "laoding files from ./data/M02/OP09/good... \n",
      "laoding files from ./data/M02/OP09/bad... \n",
      "laoding files from ./data/M03/OP09/good... \n",
      "laoding files from ./data/M03/OP09/bad... \n",
      "laoding files from ./data/M01/OP10/good... \n",
      "laoding files from ./data/M01/OP10/bad... \n",
      "laoding files from ./data/M02/OP10/good... \n",
      "laoding files from ./data/M02/OP10/bad... \n",
      "laoding files from ./data/M03/OP10/good... \n",
      "laoding files from ./data/M03/OP10/bad... \n",
      "laoding files from ./data/M01/OP11/good... \n",
      "laoding files from ./data/M01/OP11/bad... \n",
      "laoding files from ./data/M02/OP11/good... \n",
      "laoding files from ./data/M02/OP11/bad... \n",
      "laoding files from ./data/M03/OP11/good... \n",
      "laoding files from ./data/M03/OP11/bad... \n",
      "laoding files from ./data/M01/OP12/good... \n",
      "laoding files from ./data/M01/OP12/bad... \n",
      "laoding files from ./data/M02/OP12/good... \n",
      "laoding files from ./data/M02/OP12/bad... \n",
      "laoding files from ./data/M03/OP12/good... \n",
      "laoding files from ./data/M03/OP12/bad... \n",
      "laoding files from ./data/M01/OP13/good... \n",
      "laoding files from ./data/M01/OP13/bad... \n",
      "laoding files from ./data/M02/OP13/good... \n",
      "laoding files from ./data/M02/OP13/bad... \n",
      "laoding files from ./data/M03/OP13/good... \n",
      "laoding files from ./data/M03/OP13/bad... \n",
      "laoding files from ./data/M01/OP14/good... \n",
      "laoding files from ./data/M01/OP14/bad... \n",
      "laoding files from ./data/M02/OP14/good... \n",
      "laoding files from ./data/M02/OP14/bad... \n",
      "laoding files from ./data/M03/OP14/good... \n",
      "laoding files from ./data/M03/OP14/bad... \n"
     ]
    }
   ],
   "source": [
    "X_data = []\n",
    "y_data = []\n",
    "for process_name in process_names:\n",
    "    for machine in machines:\n",
    "        for label in labels:\n",
    "            data_path = os.path.join(path_to_dataset, machine, process_name, label)\n",
    "            data_list, data_label = data_loader_utils.load_tool_research_data(data_path, add_additional_label=False, label=label)\n",
    "            #concatenating\n",
    "            X_data.extend(data_list)\n",
    "            y_data.extend(data_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d56a2abe-ef39-4ec8-9cc2-23a6adffd1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_X_data = []\n",
    "good_y_data = []\n",
    "bad_X_data = []\n",
    "bad_y_data = []\n",
    "\n",
    "for i, label in enumerate(y_data):\n",
    "    if label == \"good\":\n",
    "        good_X_data.append(X_data[i])\n",
    "        good_y_data.append(label)\n",
    "    elif label == \"bad\":\n",
    "        bad_X_data.append(X_data[i])\n",
    "        bad_y_data.append(label)\n",
    "    else:\n",
    "        # Handle other labels if necessary\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2cc2ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f994ac1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "laoding files from ./data/M01/OP01/good... \n",
      "laoding files from ./data/M01/OP01/bad... \n",
      "laoding files from ./data/M01/OP11/good... \n",
      "laoding files from ./data/M01/OP11/bad... \n",
      "laoding files from ./data/M01/OP04/good... \n",
      "laoding files from ./data/M01/OP04/bad... \n",
      "laoding files from ./data/M01/OP13/good... \n",
      "laoding files from ./data/M01/OP13/bad... \n",
      "laoding files from ./data/M01/OP07/good... \n",
      "laoding files from ./data/M01/OP07/bad... \n",
      "laoding files from ./data/M01/OP06/good... \n",
      "laoding files from ./data/M01/OP06/bad... \n",
      "laoding files from ./data/M01/OP12/good... \n",
      "laoding files from ./data/M01/OP12/bad... \n",
      "laoding files from ./data/M01/OP10/good... \n",
      "laoding files from ./data/M01/OP10/bad... \n",
      "laoding files from ./data/M01/OP00/good... \n",
      "laoding files from ./data/M01/OP00/bad... \n",
      "laoding files from ./data/M01/OP05/good... \n",
      "laoding files from ./data/M01/OP05/bad... \n",
      "laoding files from ./data/M01/OP08/good... \n",
      "laoding files from ./data/M01/OP08/bad... \n",
      "laoding files from ./data/M01/OP03/good... \n",
      "laoding files from ./data/M01/OP03/bad... \n",
      "laoding files from ./data/M01/OP02/good... \n",
      "laoding files from ./data/M01/OP02/bad... \n",
      "laoding files from ./data/M01/OP09/good... \n",
      "laoding files from ./data/M01/OP09/bad... \n",
      "laoding files from ./data/M01/OP14/good... \n",
      "laoding files from ./data/M01/OP14/bad... \n",
      "laoding files from ./data/M02/OP01/good... \n",
      "laoding files from ./data/M02/OP01/bad... \n",
      "laoding files from ./data/M02/OP11/good... \n",
      "laoding files from ./data/M02/OP11/bad... \n",
      "laoding files from ./data/M02/OP04/good... \n",
      "laoding files from ./data/M02/OP04/bad... \n",
      "laoding files from ./data/M02/OP13/good... \n",
      "laoding files from ./data/M02/OP13/bad... \n",
      "laoding files from ./data/M02/OP07/good... \n",
      "laoding files from ./data/M02/OP07/bad... \n",
      "laoding files from ./data/M02/OP06/good... \n",
      "laoding files from ./data/M02/OP06/bad... \n",
      "laoding files from ./data/M02/OP12/good... \n",
      "laoding files from ./data/M02/OP12/bad... \n",
      "laoding files from ./data/M02/OP10/good... \n",
      "laoding files from ./data/M02/OP10/bad... \n",
      "laoding files from ./data/M02/OP00/good... \n",
      "laoding files from ./data/M02/OP00/bad... \n",
      "laoding files from ./data/M02/OP05/good... \n",
      "laoding files from ./data/M02/OP05/bad... \n",
      "laoding files from ./data/M02/OP08/good... \n",
      "laoding files from ./data/M02/OP08/bad... \n",
      "laoding files from ./data/M02/OP03/good... \n",
      "laoding files from ./data/M02/OP03/bad... \n",
      "laoding files from ./data/M02/OP02/good... \n",
      "laoding files from ./data/M02/OP02/bad... \n",
      "laoding files from ./data/M02/OP09/good... \n",
      "laoding files from ./data/M02/OP09/bad... \n",
      "laoding files from ./data/M02/OP14/good... \n",
      "laoding files from ./data/M02/OP14/bad... \n",
      "laoding files from ./data/M03/OP01/good... \n",
      "laoding files from ./data/M03/OP01/bad... \n",
      "laoding files from ./data/M03/OP11/good... \n",
      "laoding files from ./data/M03/OP11/bad... \n",
      "laoding files from ./data/M03/OP04/good... \n",
      "laoding files from ./data/M03/OP04/bad... \n",
      "laoding files from ./data/M03/OP13/good... \n",
      "laoding files from ./data/M03/OP13/bad... \n",
      "laoding files from ./data/M03/OP07/good... \n",
      "laoding files from ./data/M03/OP07/bad... \n",
      "laoding files from ./data/M03/OP06/good... \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define data paths and parameters\n",
    "data_root = \"./data/\"\n",
    "feature_params = {\n",
    "    \"statistical\": [\"mean\", \"std\", \"skew\", \"kurt\", \"quantile\"],\n",
    "    \"frequency_domain\": [\"dominant_freq\", \"spectral_entropy\"],\n",
    "    # Add more features and parameters as needed\n",
    "}\n",
    "window_size = 10000  # Adjust window size based on your data\n",
    "overlap = 0.5  # Adjust overlap ratio for window segments\n",
    "test_size = 0.2  # Split for training and testing\n",
    "\n",
    "# Load and preprocess data\n",
    "all_data = []\n",
    "for machine in [\"M01\", \"M02\", \"M03\"]:\n",
    "    for operation in os.listdir(os.path.join(data_root, machine)):\n",
    "        for label in [\"good\", \"bad\"]:\n",
    "            data_path = os.path.join(data_root, machine, operation, label)\n",
    "            data, labels = data_loader_utils.load_tool_research_data(data_path, label)\n",
    "            all_data.append((data, labels))\n",
    "\n",
    "# Combine data and generate features\n",
    "combined_data, combined_labels = [], []\n",
    "for data, labels in all_data:\n",
    "    X_features = extract_features(data, column_id=\"id\", column_sort=\"time\", **feature_params)\n",
    "    combined_data.append(X_features)\n",
    "    combined_labels.append(labels)\n",
    "\n",
    "X = pd.concat(combined_data, ignore_index=True)\n",
    "y = np.concatenate(combined_labels)\n",
    "\n",
    "# Oversample bad samples (optional)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=test_size)\n",
    "\n",
    "# Train an anomaly detection model\n",
    "model = IsolationForest(n_estimators=100, random_state=42)\n",
    "model.fit(X_train)\n",
    "\n",
    "# Make predictions and evaluate performance\n",
    "y_pred = model.predict(X_test)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "print(f\"F1-score: {f1:.4f}, ROC AUC: {auc:.4f}\")\n",
    "\n",
    "# Further analysis and improvements\n",
    "# ... (e.g., try different models, feature selection, hyperparameter tuning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d98651",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, SlidingWindowTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Choose your feature extractor (e.g., StandardScaler, PowerSpectralDensity)\n",
    "feature_extractor = ...\n",
    "\n",
    "# Create pipeline with windowing and feature extraction\n",
    "pipeline = Pipeline([\n",
    "    ('windowing', SlidingWindowTransformer(window_size=window_size, step=overlap)),\n",
    "    ('feature_extraction', feature_extractor)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637026cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = data_loader_utils.load_tool_research_data(data_path, label)\n",
    "features = pipeline.fit_transform(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faba53e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
