{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f52c46c-9101-4234-a034-a9eb11a1be8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import random\n",
    "import json\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import quantus\n",
    "\n",
    "#from xai_methods import GradCAMHeatmap, LaFAM, LFLRP, RELAX, Randomized7x7\n",
    "from utils import SquareCropAndResize, imagenet_transform, inverse_transform, target_transform\n",
    "from utils import evaluate, get_layer_idx\n",
    "\n",
    "\n",
    "def choose_device() -> str:\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda:0\"\n",
    "    if hasattr(torch.backends, \"mps\"):\n",
    "        if torch.backends.mps.is_available():\n",
    "            return \"mps\"\n",
    "    return \"cpu\"\n",
    "\n",
    "device = torch.device(choose_device())\n",
    "print(torch.cuda.get_device_name(device))\n",
    "\n",
    "# fix seed for reproducibility\n",
    "seed = 123\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed) # multi-GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b0801c6-94c5-4812-b207-2ee4ac57f0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def continuity(x, explainer, explanation_base, device, sigma=0.1, n_perturbations=10):\n",
    "\n",
    "\n",
    "    x = x.to(device)\n",
    "    explainer = explainer\n",
    "\n",
    "    if not isinstance(explanation_base, torch.Tensor):\n",
    "        explanation_base = torch.tensor(explanation_base, device=device, dtype=torch.float32)\n",
    "    else:\n",
    "        explanation_base = explanation_base.to(device).float()\n",
    "\n",
    "    expl_base = explanation_base\n",
    "    max_diff = 0\n",
    "\n",
    "    for _ in range(n_perturbations):\n",
    "        noise = torch.randn_like(x) * sigma\n",
    "        x_perturbed = (x + noise).detach()\n",
    "        x_perturbed.requires_grad_()\n",
    "    \n",
    "        expl_perturbed = explainer(x_perturbed)\n",
    "        \n",
    "        if not isinstance(expl_perturbed, torch.Tensor):\n",
    "            expl_perturbed = torch.tensor(expl_perturbed, device=device, dtype=torch.float32)\n",
    "        else:\n",
    "            expl_perturbed = expl_perturbed.to(device).float()\n",
    "\n",
    "        \n",
    "        while expl_perturbed.dim() > 2:\n",
    "            expl_perturbed = expl_perturbed.squeeze(0)\n",
    "\n",
    "\n",
    "        if expl_perturbed.shape[-2:] != expl_base.shape[-2:]:\n",
    "            \n",
    "            expl_perturbed = expl_perturbed.unsqueeze(0).unsqueeze(0)  # shape: (1,1,H,W)\n",
    "            expl_perturbed = torch.nn.functional.interpolate(\n",
    "                expl_perturbed,\n",
    "                size=expl_base.shape[-2:],  # target spatial size\n",
    "                mode='nearest'\n",
    "            )\n",
    "            expl_perturbed = expl_perturbed.squeeze(0).squeeze(0)\n",
    "\n",
    "      \n",
    "        expl_perturbed = (expl_perturbed - expl_perturbed.min()) / (expl_perturbed.max() - expl_perturbed.min() + 1e-8)\n",
    "        expl_base_norm = (expl_base - expl_base.min()) / (expl_base.max() - expl_base.min() + 1e-8)\n",
    "\n",
    "        \n",
    "        diff = torch.abs(expl_base_norm - expl_perturbed).max().item()\n",
    "        if diff > max_diff:\n",
    "            max_diff = diff\n",
    "\n",
    "    \n",
    "    continuity_score = 1 - max_diff\n",
    "    return continuity_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5edc55cd-0a31-4ac6-a543-d82cc169311f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Consistency Metric\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mitertools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m combinations\n\u001b[1;32m      7\u001b[0m method_explanations \u001b[38;5;241m=\u001b[39m {name: [] \u001b[38;5;28;01mfor\u001b[39;00m name, _ \u001b[38;5;129;01min\u001b[39;00m xai_methods}\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# Consistency Metric\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "method_explanations = {name: [] for name, _ in xai_methods}\n",
    "method_predictions = []\n",
    "\n",
    "df_sample = pascal_df.sample(10)\n",
    "print(f\"Evaluating {len(df_sample)} samples...\")\n",
    "\n",
    "for i, row in enumerate(tqdm(df_sample.itertuples(), total=len(df_sample))):\n",
    "    img_tensor, seg_img = pascal_ds[int(row.dataset_idx)]\n",
    "    img_tensor = img_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = resnet(img_tensor).argmax().item()\n",
    "    method_predictions.append(pred)\n",
    "\n",
    "    for name, method in xai_methods:\n",
    "        try:\n",
    "            heatmap = method(img_tensor)\n",
    "            heatmap = torch.relu(heatmap)\n",
    "            heatmap = heatmap / (heatmap.max() + 1e-8)\n",
    "\n",
    "            heatmap_np = heatmap.squeeze().detach().cpu().numpy()\n",
    "\n",
    "            # Normalize heatmap\n",
    "            heatmap_np = (heatmap_np - np.min(heatmap_np)) / (np.max(heatmap_np) - np.min(heatmap_np) + 1e-8)\n",
    "\n",
    "            # Check for NaNs or infs\n",
    "            if np.isnan(heatmap_np).any() or np.isinf(heatmap_np).any():\n",
    "                raise ValueError(\"Heatmap contains NaN or inf values.\")\n",
    "\n",
    "          \n",
    "            heatmap_pil = Image.fromarray((heatmap_np * 255).astype(np.uint8))\n",
    "            heatmap_pil = heatmap_pil.resize((224, 224), Image.NEAREST)\n",
    "            heatmap = np.array(heatmap_pil).astype(np.float32).flatten()\n",
    "\n",
    "            method_explanations[name].append(heatmap)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[{name}] Sample {i} failed: {e}\")\n",
    "            method_explanations[name].append(None)\n",
    "            continue\n",
    "\n",
    "\n",
    "def compute_consistency_from_vectors(explanations, predictions, similarity_threshold=0.9):\n",
    "    total_pairs = 0\n",
    "    consistent_pairs = 0\n",
    "\n",
    "    for i, j in combinations(range(len(explanations)), 2):\n",
    "        exp_i, exp_j = explanations[i], explanations[j]\n",
    "        if exp_i is None or exp_j is None:\n",
    "            continue\n",
    "\n",
    "        sim = cosine_similarity(\n",
    "            exp_i.reshape(1, -1),\n",
    "            exp_j.reshape(1, -1)\n",
    "        )[0][0]\n",
    "\n",
    "        if sim >= similarity_threshold:\n",
    "            total_pairs += 1\n",
    "            same = predictions[i] == predictions[j]\n",
    "            consistent_pairs += int(same)\n",
    "            #print(f\"Pair ({i},{j}): sim={sim:.3f}, same_pred={same}\")\n",
    "\n",
    "    if total_pairs == 0:\n",
    "        return None\n",
    "    return consistent_pairs / total_pairs\n",
    "\n",
    "\n",
    "consistency_results = []\n",
    "for name in method_explanations:\n",
    "    explanations = method_explanations[name]\n",
    "    valid_expls = sum([e is not None for e in explanations])\n",
    "    #print(f\"\\n{name}: {valid_expls} valid explanations\")\n",
    "\n",
    "    consistency = compute_consistency_from_vectors(\n",
    "        explanations, method_predictions, similarity_threshold=0.9\n",
    "    )\n",
    "\n",
    "    consistency_results.append({\n",
    "        \"xai_method\": name,\n",
    "        \"consistency\": consistency\n",
    "    })\n",
    "\n",
    "df_consistency = pd.DataFrame(consistency_results)\n",
    "print(\"Consistency Results:\")\n",
    "print(df_consistency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b27fc63-5d34-431b-8fa4-26c98940c48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def local_lipschitz(\n",
    "    x,\n",
    "    explainer,\n",
    "    explanation_base,\n",
    "    n_samples=50,\n",
    "    noise_std=0.01,\n",
    "    norm_type=2,\n",
    "    device='cpu',\n",
    "    random_seed=None\n",
    "):\n",
    "   \n",
    "    if random_seed is not None:\n",
    "        torch.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "    x = x.to(device)\n",
    "    if not torch.is_tensor(explanation_base):\n",
    "        explanation_base = torch.tensor(explanation_base, device=device)\n",
    "    explanation_base = explanation_base.unsqueeze(0) if explanation_base.dim() == 2 else explanation_base\n",
    "\n",
    "    max_lipschitz = 0.0\n",
    "\n",
    "    for _ in range(n_samples):\n",
    "        noise = torch.randn_like(x) * noise_std\n",
    "        x_perturbed = (x + noise).detach().clone().requires_grad_()\n",
    "\n",
    "        # Get explanation for perturbed input\n",
    "        explanation_perturbed = explainer(x_perturbed)\n",
    "        explanation_perturbed = explanation_perturbed.to(device) \n",
    "        explanation_perturbed = torch.relu(explanation_perturbed)\n",
    "        explanation_perturbed = explanation_perturbed / (explanation_perturbed.max() + 1e-8)\n",
    "        explanation_perturbed = explanation_perturbed.squeeze()\n",
    "            '''\n",
    "         Maryam to in ghesmat chon man ba image haie mokhatlef kar mikardm va size ha motefavet dasht, inja to in \n",
    "         \n",
    "         if explanation_perturbed.shape != explanation_base.shape:\n",
    "            explanation_perturbed = torch.nn.functional.interpolate(\n",
    "                explanation_perturbed.unsqueeze(0).unsqueeze(0),\n",
    "                size=explanation_base.shape[-2:],\n",
    "                mode='nearest'\n",
    "            ).squeeze()\n",
    "\n",
    "         hamashon ro ie size mikardm, shayad in ghesmat fght be karet nayad o aziatete kone, inja ro havest bashe, to baghie ie metric ha ham bayad ie hamchinchizi bebini\n",
    "         \n",
    "         \n",
    "    '''\n",
    "\n",
    "        # Resize if needed\n",
    "        if explanation_perturbed.shape != explanation_base.shape:\n",
    "            explanation_perturbed = F.interpolate(\n",
    "                explanation_perturbed.unsqueeze(0).unsqueeze(0),\n",
    "                size=explanation_base.shape[-2:],\n",
    "                mode='nearest'\n",
    "            ).squeeze()\n",
    "            explanation_perturbed = explanation_perturbed.to(device)\n",
    "        # Compute norm of explanation difference\n",
    "        diff_exp = explanation_perturbed - explanation_base\n",
    "        norm_exp = torch.norm(diff_exp.view(-1), p=norm_type)\n",
    "\n",
    "        # Compute norm of input difference\n",
    "        diff_x = x_perturbed - x\n",
    "        norm_x = torch.norm(diff_x.view(-1), p=norm_type)\n",
    "\n",
    "        if norm_x.item() > 0:\n",
    "            lipschitz_estimate = (norm_exp / norm_x).item()\n",
    "            if lipschitz_estimate > max_lipschitz:\n",
    "                max_lipschitz = lipschitz_estimate\n",
    "\n",
    "    return max_lipschitz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497c764b-c50c-4a6e-946a-f6ef2ce6496b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_sensitivity(\n",
    "    x,\n",
    "    explainer,\n",
    "    explanation_base,\n",
    "    n_samples=50,\n",
    "    noise_std=0.01,\n",
    "    norm_type=1,\n",
    "    device='cpu',\n",
    "    random_seed=None\n",
    "):\n",
    "    if random_seed is not None:\n",
    "        torch.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "    x = x.to(device)\n",
    "\n",
    "    if not torch.is_tensor(explanation_base):\n",
    "        explanation_base = torch.tensor(explanation_base, device=device)\n",
    "    else:\n",
    "        explanation_base = explanation_base.to(device)\n",
    "    explanation_base = explanation_base.unsqueeze(0)\n",
    "\n",
    "    sensitivities = []\n",
    "        '''\n",
    "         Maryam to in ghesmat chon man ba image haie mokhatlef kar mikardm va size ha motefavet dasht, inja to in \n",
    "         \n",
    "         if explanation_perturbed.shape != explanation_base.shape:\n",
    "            explanation_perturbed = torch.nn.functional.interpolate(\n",
    "                explanation_perturbed.unsqueeze(0).unsqueeze(0),\n",
    "                size=explanation_base.shape[-2:],\n",
    "                mode='nearest'\n",
    "            ).squeeze()\n",
    "\n",
    "         hamashon ro ie size mikardm, shayad in ghesmat fght be karet nayad o aziatete kone, inja ro havest bashe, to baghie ie metric ha ham bayad ie hamchinchizi bebini\n",
    "         \n",
    "         \n",
    "    '''\n",
    "    for _ in range(n_samples):\n",
    "        noise = torch.randn_like(x) * noise_std\n",
    "        x_perturbed = (x + noise).detach().clone().requires_grad_()\n",
    "\n",
    "        explanation_perturbed = explainer(x_perturbed)\n",
    "        explanation_perturbed = explanation_perturbed.to(device)  # ensure device alignment\n",
    "\n",
    "        explanation_perturbed = torch.relu(explanation_perturbed)\n",
    "        explanation_perturbed = explanation_perturbed / (explanation_perturbed.max() + 1e-8)\n",
    "        explanation_perturbed = explanation_perturbed.squeeze()\n",
    "\n",
    "        if explanation_perturbed.shape != explanation_base.shape:\n",
    "            explanation_perturbed = torch.nn.functional.interpolate(\n",
    "                explanation_perturbed.unsqueeze(0).unsqueeze(0),\n",
    "                size=explanation_base.shape[-2:],\n",
    "                mode='nearest'\n",
    "            ).squeeze()\n",
    "            explanation_perturbed = explanation_perturbed.to(device)\n",
    "\n",
    "        diff = explanation_perturbed - explanation_base\n",
    "        diff_norm = torch.norm(diff.view(-1), p=norm_type)\n",
    "        delta_norm = torch.norm(noise.view(-1), p=norm_type)\n",
    "\n",
    "        if delta_norm.item() > 0:\n",
    "            sensitivities.append((diff_norm / delta_norm).item())\n",
    "\n",
    "    return np.mean(sensitivities)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
